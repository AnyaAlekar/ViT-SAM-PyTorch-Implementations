{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8ec70e",
      "metadata": {
        "id": "3d8ec70e"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision\n",
        "\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eede0fcd",
      "metadata": {
        "id": "eede0fcd"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0\n",
        "        self.grid_size = img_size // patch_size\n",
        "        self.num_patches = self.grid_size * self.grid_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0, qkv_bias=True, drop=0., attn_drop=0., norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(dim, mlp_hidden_dim, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10, embed_dim=256, depth=8, num_heads=8, mlp_ratio=4.0, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, drop=drop_rate) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        cls = x[:, 0]\n",
        "        return self.head(cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1092e882",
      "metadata": {
        "id": "1092e882"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(batch_size=128, num_workers=2):\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std = (0.2470, 0.2435, 0.2616)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        preds = model(imgs).argmax(1)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756a0082",
      "metadata": {
        "id": "756a0082"
      },
      "outputs": [],
      "source": [
        "def train_vit(epochs=200, batch_size=128, lr=3e-4, weight_decay=0.05, patch_size=4, embed_dim=256, depth=8, heads=8, device='cuda'):\n",
        "    set_seed(42)\n",
        "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "    train_loader, test_loader = get_data_loaders(batch_size=batch_size)\n",
        "    model = VisionTransformer(embed_dim=embed_dim, patch_size=patch_size, depth=depth, num_heads=heads)\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        acc = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch:03d}: Test Accuracy = {acc*100:.2f}%\")\n",
        "        best_acc = max(best_acc, acc)\n",
        "    print(f\"Best Accuracy: {best_acc*100:.2f}%\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd7ab56",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdd7ab56",
        "outputId": "4788442e-76a4-4d10-dd09-f8853229a019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001: Test Accuracy = 38.03%\n",
            "Epoch 002: Test Accuracy = 49.34%\n",
            "Epoch 003: Test Accuracy = 53.88%\n",
            "Epoch 004: Test Accuracy = 56.74%\n",
            "Epoch 005: Test Accuracy = 60.80%\n",
            "Epoch 006: Test Accuracy = 63.33%\n",
            "Epoch 007: Test Accuracy = 66.29%\n",
            "Epoch 008: Test Accuracy = 67.66%\n",
            "Epoch 009: Test Accuracy = 68.92%\n",
            "Epoch 010: Test Accuracy = 70.25%\n",
            "Epoch 011: Test Accuracy = 70.68%\n",
            "Epoch 012: Test Accuracy = 73.02%\n",
            "Epoch 013: Test Accuracy = 73.02%\n",
            "Epoch 014: Test Accuracy = 73.91%\n",
            "Epoch 015: Test Accuracy = 75.71%\n",
            "Epoch 016: Test Accuracy = 74.62%\n",
            "Epoch 017: Test Accuracy = 75.85%\n",
            "Epoch 018: Test Accuracy = 75.17%\n",
            "Epoch 019: Test Accuracy = 75.90%\n",
            "Epoch 020: Test Accuracy = 76.24%\n",
            "Epoch 021: Test Accuracy = 77.00%\n",
            "Epoch 022: Test Accuracy = 78.13%\n",
            "Epoch 023: Test Accuracy = 77.94%\n",
            "Epoch 024: Test Accuracy = 78.18%\n",
            "Epoch 025: Test Accuracy = 78.72%\n",
            "Epoch 026: Test Accuracy = 78.78%\n",
            "Epoch 027: Test Accuracy = 78.52%\n",
            "Epoch 028: Test Accuracy = 79.24%\n",
            "Epoch 029: Test Accuracy = 78.99%\n",
            "Epoch 030: Test Accuracy = 79.56%\n",
            "Epoch 031: Test Accuracy = 80.16%\n",
            "Epoch 032: Test Accuracy = 79.74%\n",
            "Epoch 033: Test Accuracy = 79.54%\n",
            "Epoch 034: Test Accuracy = 79.29%\n",
            "Epoch 035: Test Accuracy = 79.91%\n",
            "Epoch 036: Test Accuracy = 79.77%\n",
            "Epoch 037: Test Accuracy = 79.22%\n",
            "Epoch 038: Test Accuracy = 78.88%\n",
            "Epoch 039: Test Accuracy = 79.52%\n",
            "Epoch 040: Test Accuracy = 80.17%\n",
            "Epoch 041: Test Accuracy = 80.01%\n",
            "Epoch 042: Test Accuracy = 79.90%\n",
            "Epoch 043: Test Accuracy = 80.09%\n",
            "Epoch 044: Test Accuracy = 79.96%\n",
            "Epoch 045: Test Accuracy = 80.19%\n",
            "Epoch 046: Test Accuracy = 79.81%\n",
            "Epoch 047: Test Accuracy = 79.48%\n",
            "Epoch 048: Test Accuracy = 80.41%\n",
            "Epoch 049: Test Accuracy = 80.14%\n",
            "Epoch 050: Test Accuracy = 79.68%\n",
            "Epoch 051: Test Accuracy = 80.25%\n",
            "Epoch 052: Test Accuracy = 79.84%\n",
            "Epoch 053: Test Accuracy = 79.81%\n",
            "Epoch 054: Test Accuracy = 80.24%\n",
            "Epoch 055: Test Accuracy = 79.69%\n",
            "Epoch 056: Test Accuracy = 79.61%\n",
            "Epoch 057: Test Accuracy = 79.49%\n",
            "Epoch 058: Test Accuracy = 80.55%\n",
            "Epoch 059: Test Accuracy = 79.14%\n",
            "Epoch 060: Test Accuracy = 80.53%\n",
            "Epoch 061: Test Accuracy = 79.68%\n",
            "Epoch 062: Test Accuracy = 79.24%\n",
            "Epoch 063: Test Accuracy = 80.38%\n",
            "Epoch 064: Test Accuracy = 80.43%\n",
            "Epoch 065: Test Accuracy = 80.47%\n",
            "Epoch 066: Test Accuracy = 80.31%\n",
            "Epoch 067: Test Accuracy = 80.69%\n",
            "Epoch 068: Test Accuracy = 80.18%\n",
            "Epoch 069: Test Accuracy = 80.71%\n",
            "Epoch 070: Test Accuracy = 80.46%\n",
            "Epoch 071: Test Accuracy = 79.09%\n",
            "Epoch 072: Test Accuracy = 79.99%\n",
            "Epoch 073: Test Accuracy = 80.39%\n",
            "Epoch 074: Test Accuracy = 79.61%\n",
            "Epoch 075: Test Accuracy = 80.15%\n",
            "Epoch 076: Test Accuracy = 80.05%\n",
            "Epoch 077: Test Accuracy = 79.49%\n",
            "Epoch 078: Test Accuracy = 79.85%\n",
            "Epoch 079: Test Accuracy = 80.18%\n",
            "Epoch 080: Test Accuracy = 80.43%\n",
            "Epoch 081: Test Accuracy = 79.62%\n",
            "Epoch 082: Test Accuracy = 80.26%\n",
            "Epoch 083: Test Accuracy = 80.50%\n",
            "Epoch 084: Test Accuracy = 80.88%\n",
            "Epoch 085: Test Accuracy = 80.13%\n",
            "Epoch 086: Test Accuracy = 80.46%\n",
            "Epoch 087: Test Accuracy = 80.63%\n",
            "Epoch 088: Test Accuracy = 80.60%\n",
            "Epoch 089: Test Accuracy = 80.76%\n",
            "Epoch 090: Test Accuracy = 80.63%\n",
            "Epoch 091: Test Accuracy = 80.28%\n",
            "Epoch 092: Test Accuracy = 80.59%\n",
            "Epoch 093: Test Accuracy = 80.80%\n",
            "Epoch 094: Test Accuracy = 79.25%\n",
            "Epoch 095: Test Accuracy = 80.28%\n",
            "Epoch 096: Test Accuracy = 79.76%\n",
            "Epoch 097: Test Accuracy = 79.33%\n",
            "Epoch 098: Test Accuracy = 80.29%\n",
            "Epoch 099: Test Accuracy = 80.13%\n",
            "Epoch 100: Test Accuracy = 81.43%\n",
            "Epoch 101: Test Accuracy = 79.87%\n",
            "Epoch 102: Test Accuracy = 80.24%\n",
            "Epoch 103: Test Accuracy = 80.53%\n",
            "Epoch 104: Test Accuracy = 80.41%\n",
            "Epoch 105: Test Accuracy = 80.59%\n",
            "Epoch 106: Test Accuracy = 80.55%\n",
            "Epoch 107: Test Accuracy = 80.87%\n",
            "Epoch 108: Test Accuracy = 80.25%\n",
            "Epoch 109: Test Accuracy = 80.88%\n",
            "Epoch 110: Test Accuracy = 80.19%\n",
            "Epoch 111: Test Accuracy = 80.02%\n",
            "Epoch 112: Test Accuracy = 80.87%\n",
            "Epoch 113: Test Accuracy = 80.12%\n",
            "Epoch 114: Test Accuracy = 80.27%\n",
            "Epoch 115: Test Accuracy = 80.71%\n",
            "Epoch 116: Test Accuracy = 80.55%\n",
            "Epoch 117: Test Accuracy = 80.61%\n",
            "Epoch 118: Test Accuracy = 80.51%\n",
            "Epoch 119: Test Accuracy = 80.72%\n",
            "Epoch 120: Test Accuracy = 80.32%\n",
            "Epoch 121: Test Accuracy = 80.16%\n",
            "Epoch 122: Test Accuracy = 81.23%\n",
            "Epoch 123: Test Accuracy = 80.28%\n",
            "Epoch 124: Test Accuracy = 80.11%\n",
            "Epoch 125: Test Accuracy = 80.60%\n",
            "Epoch 126: Test Accuracy = 79.83%\n",
            "Epoch 127: Test Accuracy = 80.43%\n",
            "Epoch 128: Test Accuracy = 80.81%\n",
            "Epoch 129: Test Accuracy = 80.53%\n",
            "Epoch 130: Test Accuracy = 80.78%\n",
            "Epoch 131: Test Accuracy = 80.60%\n",
            "Epoch 132: Test Accuracy = 80.50%\n",
            "Epoch 133: Test Accuracy = 80.02%\n",
            "Epoch 134: Test Accuracy = 80.92%\n",
            "Epoch 135: Test Accuracy = 80.76%\n",
            "Epoch 136: Test Accuracy = 80.21%\n",
            "Epoch 137: Test Accuracy = 80.33%\n",
            "Epoch 138: Test Accuracy = 80.96%\n",
            "Epoch 139: Test Accuracy = 80.34%\n",
            "Epoch 140: Test Accuracy = 79.79%\n",
            "Epoch 141: Test Accuracy = 81.01%\n",
            "Epoch 142: Test Accuracy = 79.61%\n",
            "Epoch 143: Test Accuracy = 80.44%\n",
            "Epoch 144: Test Accuracy = 80.17%\n",
            "Epoch 145: Test Accuracy = 80.80%\n",
            "Epoch 146: Test Accuracy = 79.99%\n",
            "Epoch 147: Test Accuracy = 80.03%\n",
            "Epoch 148: Test Accuracy = 80.25%\n",
            "Epoch 149: Test Accuracy = 80.67%\n",
            "Epoch 150: Test Accuracy = 80.61%\n",
            "Epoch 151: Test Accuracy = 80.17%\n",
            "Epoch 152: Test Accuracy = 80.81%\n",
            "Epoch 153: Test Accuracy = 80.20%\n",
            "Epoch 154: Test Accuracy = 80.60%\n",
            "Epoch 155: Test Accuracy = 80.35%\n",
            "Epoch 156: Test Accuracy = 80.54%\n",
            "Epoch 157: Test Accuracy = 80.67%\n",
            "Epoch 158: Test Accuracy = 80.25%\n",
            "Epoch 159: Test Accuracy = 80.95%\n",
            "Epoch 160: Test Accuracy = 79.54%\n",
            "Epoch 161: Test Accuracy = 80.71%\n",
            "Epoch 162: Test Accuracy = 80.16%\n",
            "Epoch 163: Test Accuracy = 81.19%\n",
            "Epoch 164: Test Accuracy = 80.55%\n",
            "Epoch 165: Test Accuracy = 80.67%\n",
            "Epoch 166: Test Accuracy = 80.44%\n",
            "Epoch 167: Test Accuracy = 81.00%\n",
            "Epoch 168: Test Accuracy = 80.87%\n",
            "Epoch 169: Test Accuracy = 80.90%\n",
            "Epoch 170: Test Accuracy = 80.28%\n",
            "Epoch 171: Test Accuracy = 80.65%\n",
            "Epoch 172: Test Accuracy = 80.09%\n"
          ]
        }
      ],
      "source": [
        "model = train_vit(epochs=200, batch_size=256, lr=3e-4, embed_dim=256, depth=8, heads=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Q6P5p1Wsc9a",
      "metadata": {
        "id": "0Q6P5p1Wsc9a"
      },
      "source": [
        "##### Accuracy\n",
        "1. epochs=50, batch_size=128, lr=3e-4, embed_dim=256, depth=8, heads=8: 80.18%\n",
        "2. epochs=100, batch_size=256, lr=3e-4, embed_dim=256, depth=8, heads=8: 80.35%\n",
        "3. epochs=100, batch_size=128, lr=3e-4, embed_dim=256, depth=8, heads=8: 80.89%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}